{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Load images function\n",
    "def load_images(folder):\n",
    "    images = []\n",
    "    transform = transforms.Compose([transforms.Resize((100, 100)), transforms.ToTensor()])\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.png'):\n",
    "            img = Image.open(os.path.join(folder, filename)).convert('RGB')\n",
    "            img = transform(img)\n",
    "            images.append(img)\n",
    "    return torch.stack(images)\n",
    "\n",
    "# Load the .npy file to inspect its contents\n",
    "trajectories_data = np.load('trajectories_3.npy', allow_pickle=True)\n",
    "print(type(trajectories_data))\n",
    "print(trajectories_data.shape)\n",
    "print(trajectories_data)\n",
    "\n",
    "# Define the Resnet_Visual_Encoder function\n",
    "class ResNet18VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18VisualEncoder, self).__init__()\n",
    "        self.resnet18 = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.resnet18 = nn.Sequential(*list(self.resnet18.children())[:-2])\n",
    "\n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.resnet18(images)\n",
    "        return embeddings\n",
    "\n",
    "def Resnet_Visual_Encoder(images):\n",
    "    encoder = ResNet18VisualEncoder()\n",
    "    latent_embeddings = encoder(images)\n",
    "    return latent_embeddings\n",
    "\n",
    "# Define the Pathl function\n",
    "def Pathl(data):\n",
    "    trajectories = [d['trajectory'] for d in data]\n",
    "    lengths = [len(traj) for traj in trajectories]\n",
    "    return lengths\n",
    "\n",
    "# Define the adding_Gaussian_noise function\n",
    "# Cosine noise scheduler\n",
    "def cosine_noise_scheduler(T, beta_start=0.0001, beta_end=0.02):\n",
    "    return np.cos((np.linspace(0, T, T) + 0.008) / (1.008) * np.pi * 0.5) ** 2 * (beta_end - beta_start) + beta_start\n",
    "\n",
    "# Adding Gaussian noise function\n",
    "def adding_Gaussian_noise(lengths, trajectories):\n",
    "    noisy_trajectories = []\n",
    "    T = 1000  # Number of diffusion steps\n",
    "    beta_schedule = cosine_noise_scheduler(T)\n",
    "    for i, length in enumerate(lengths):\n",
    "        trajectory = trajectories[i]\n",
    "        noise = np.random.normal(0, 1, (length, 2))\n",
    "        noisy_trajectory = np.array(trajectory) + noise * beta_schedule[:length, None]\n",
    "        noisy_trajectories.append(noisy_trajectory)\n",
    "    return noisy_trajectories\n",
    "\n",
    "# Define the noise_prediction_network function\n",
    "# Noise prediction network class\n",
    "class NoisePredictionNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(NoisePredictionNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# FiLM layer class\n",
    "class FiLM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(FiLM, self).__init__()\n",
    "        self.gamma = nn.Linear(in_channels, out_channels)\n",
    "        self.beta = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        gamma = self.gamma(cond)\n",
    "        beta = self.beta(cond)\n",
    "        return gamma * x + beta\n",
    "\n",
    "# Temporal CNN with FiLM class\n",
    "class TemporalCNNFiLM(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(TemporalCNNFiLM, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)\n",
    "        self.film1 = FiLM(latent_dim, 64)\n",
    "        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n",
    "        self.film2 = FiLM(latent_dim, 32)\n",
    "        self.conv3 = nn.Conv1d(32, 2, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(self.film1(x, cond))\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(self.film2(x, cond))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "# Train noise prediction network function\n",
    "def train_noise_prediction_network(latent_embeddings, start_goals, noisy_trajectories, actual_noises, epochs=2, lr=0.01):\n",
    "    latent_dim = latent_embeddings.shape[1] * latent_embeddings.shape[2] * latent_embeddings.shape[3]\n",
    "    model = TemporalCNNFiLM(input_dim=2, latent_dim=latent_dim + 4)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(noisy_trajectories)):\n",
    "            latent_embedding = latent_embeddings[i].view(-1)  # Flatten to 1D tensor\n",
    "            start_goal_flat = start_goals[i].view(-1)  # Flatten start_goal to 1D tensor\n",
    "            cond = torch.cat((latent_embedding, start_goal_flat), dim=0).float()  # Combine latents and start_goal\n",
    "            noisy_traj = noisy_trajectories[i].permute(1, 0).unsqueeze(0)  # [1, 2, T]\n",
    "            actual_noise = actual_noises[i].permute(1, 0).unsqueeze(0)  # [1, 2, T]\n",
    "\n",
    "            predictions = model(noisy_traj, cond.unsqueeze(0))\n",
    "            loss = criterion(predictions, actual_noise)  # Compute loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# DiPPeR model class using Temporal CNN with FiLM\n",
    "class DiPPeR(nn.Module):\n",
    "    def __init__(self, noise_prediction_network):\n",
    "        super(DiPPeR, self).__init__()\n",
    "        self.noise_prediction_network = noise_prediction_network\n",
    "\n",
    "    def forward(self, O, noisy_trajectory, path_length, start_goal):\n",
    "        denoised_trajectory = noisy_trajectory\n",
    "        latent_embedding = O.view(-1)  # Flatten latent embedding\n",
    "        start_goal_flat = start_goal.view(-1)  # Flatten start_goal\n",
    "        cond = torch.cat((latent_embedding, start_goal_flat), dim=0).float()  # Combine latents and start_goal\n",
    "\n",
    "        for t in reversed(range(path_length)):\n",
    "            traj_point = denoised_trajectory[t].view(-1)  # Flatten trajectory point\n",
    "            traj_point = traj_point.unsqueeze(0).unsqueeze(0)  # [1, 1, 2]\n",
    "            noise_pred = self.noise_prediction_network(traj_point.permute(0, 2, 1), cond.unsqueeze(0))\n",
    "            denoised_trajectory[t] = denoised_trajectory[t] - noise_pred.squeeze(0).permute(1, 0)\n",
    "\n",
    "        return denoised_trajectory\n",
    "\n",
    "# Load data\n",
    "images = load_images('./maze_maps3')\n",
    "latent_embeddings = Resnet_Visual_Encoder(images)\n",
    "\n",
    "# Load trajectories data\n",
    "data = np.load('trajectories_3.npy', allow_pickle=True)\n",
    "\n",
    "start_goals = []\n",
    "trajectories_list = []\n",
    "for d in data:\n",
    "    start = torch.tensor(d['start'])\n",
    "    goal = torch.tensor(d['goal'])\n",
    "    start_goal = torch.cat((start, goal))  # Concatenate start and goal tensors\n",
    "    start_goals.append(start_goal)\n",
    "    trajectories_list.append(torch.tensor(d['trajectory']))\n",
    "\n",
    "# Calculate lengths and add Gaussian noise\n",
    "lengths = Pathl(data)\n",
    "noisy_trajectories = adding_Gaussian_noise(lengths, trajectories_list)\n",
    "\n",
    "# Ensure all tensors have the same number of elements\n",
    "if len(latent_embeddings) != len(start_goals):\n",
    "    print(f\"Mismatch in the number of images ({len(latent_embeddings)}) and trajectories ({len(start_goals)}). Adjusting...\")\n",
    "\n",
    "    # Adjust latent embeddings to match the number of start_goals\n",
    "    latent_embeddings = latent_embeddings[:len(start_goals)]\n",
    "\n",
    "assert len(latent_embeddings) == len(start_goals) == len(noisy_trajectories), \"Mismatch in number of elements\"\n",
    "\n",
    "# Convert data to tensors\n",
    "noisy_trajectories_tensor = pad_sequence([torch.tensor(traj) for traj in noisy_trajectories], batch_first=True)\n",
    "actual_noises = pad_sequence([torch.tensor(traj) - torch.tensor(trajectories_list[i]) for i, traj in enumerate(noisy_trajectories)], batch_first=True)\n",
    "\n",
    "# Train the noise prediction network\n",
    "noise_prediction_model = train_noise_prediction_network(latent_embeddings, start_goals, noisy_trajectories_tensor, actual_noises)\n",
    "\n",
    "# Create and use the DiPPeR model\n",
    "dipper_model = DiPPeR(noise_prediction_model)\n",
    "final_trajectories = [dipper_model(latent_embeddings[i], noisy_trajectories_tensor[i], lengths[i], start_goals[i]) for i in range(len(noisy_trajectories_tensor))]\n",
    "\n",
    "# Plot images with trajectories\n",
    "def plot_images_with_trajectories(images, trajectories, output_folder='output_images'):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    transform = transforms.ToPILImage()\n",
    "\n",
    "    for i, (image, trajectory) in enumerate(zip(images, trajectories)):\n",
    "        img = transform(image)\n",
    "        plt.figure()\n",
    "        plt.imshow(img)\n",
    "\n",
    "        trajectory = trajectory.detach().cpu().numpy()\n",
    "        plt.plot(trajectory[:, 0], trajectory[:, 1], marker='o', color='r', linestyle='-')\n",
    "\n",
    "        plt.title(f'Image with Denoised Trajectory {i}')\n",
    "        plt.axis('off')\n",
    "        plt.savefig(os.path.join(output_folder, f'image_with_trajectory_{i}.png'))\n",
    "        plt.close()\n",
    "\n",
    "plot_images_with_trajectories(images, final_trajectories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(5,)\n",
      "[{'map_name': 'maze_1.png', 'start': (31, 53), 'goal': (15, 56), 'trajectory': [(31, 54), (31, 55), (32, 55), (33, 55), (34, 55), (35, 55), (36, 55), (37, 55), (38, 55), (39, 55), (40, 55), (41, 55), (42, 55), (43, 55), (44, 55), (45, 55), (46, 55), (47, 55), (48, 55), (49, 55), (50, 55), (51, 55), (52, 55), (53, 55), (54, 55), (55, 55), (56, 55), (57, 55), (58, 55), (59, 55), (60, 55), (60, 56), (60, 57), (60, 58), (60, 59), (60, 60), (60, 61), (60, 62), (60, 63), (60, 64), (60, 65), (60, 66), (60, 67), (60, 68), (60, 69), (60, 70), (60, 71), (60, 72), (60, 73), (60, 74), (60, 75), (60, 76), (59, 76), (58, 76), (57, 76), (56, 76), (55, 76), (54, 76), (53, 76), (52, 76), (52, 75), (52, 74), (52, 73), (52, 72), (52, 71), (52, 70), (52, 69), (52, 68), (52, 67), (52, 66), (52, 65), (52, 64), (52, 63), (51, 63), (50, 63), (49, 63), (48, 63), (47, 63), (46, 63), (45, 63), (44, 63), (43, 63), (42, 63), (41, 63), (40, 63), (39, 63), (38, 63), (37, 63), (36, 63), (36, 64), (36, 65), (36, 66), (36, 67), (36, 68), (35, 68), (34, 68), (33, 68), (32, 68), (31, 68), (30, 68), (29, 68), (28, 68), (27, 68), (26, 68), (25, 68), (24, 68), (23, 68), (23, 67), (23, 66), (23, 65), (23, 64), (22, 64), (22, 63), (21, 63), (21, 62), (20, 62), (20, 61), (19, 61), (19, 60), (18, 60), (17, 60), (16, 60), (15, 60), (15, 59), (15, 58), (15, 57), (15, 56)]}\n",
      " {'map_name': 'maze_2.png', 'start': (66, 79), 'goal': (74, 87), 'trajectory': [(67, 79), (68, 79), (68, 80), (68, 81), (68, 82), (69, 82), (69, 83), (70, 83), (70, 84), (71, 84), (71, 85), (72, 85), (72, 86), (73, 86), (73, 87), (74, 87)]}\n",
      " {'map_name': 'maze_3.png', 'start': (47, 6), 'goal': (34, 29), 'trajectory': [(47, 7), (47, 8), (47, 9), (47, 10), (47, 11), (47, 12), (47, 13), (47, 14), (47, 15), (46, 15), (45, 15), (44, 15), (43, 15), (42, 15), (41, 15), (40, 15), (39, 15), (39, 16), (39, 17), (39, 18), (39, 19), (39, 20), (39, 21), (39, 22), (39, 23), (39, 24), (38, 24), (38, 25), (37, 25), (37, 26), (36, 26), (36, 27), (36, 28), (35, 28), (34, 28), (34, 29)]}\n",
      " {'map_name': 'maze_4.png', 'start': (55, 52), 'goal': (86, 67), 'trajectory': [(55, 53), (55, 54), (55, 55), (54, 55), (53, 55), (52, 55), (51, 55), (50, 55), (49, 55), (48, 55), (47, 55), (46, 55), (45, 55), (44, 55), (43, 55), (42, 55), (41, 55), (40, 55), (39, 55), (39, 56), (39, 57), (39, 58), (39, 59), (39, 60), (40, 60), (41, 60), (42, 60), (43, 60), (44, 60), (45, 60), (46, 60), (47, 60), (47, 61), (47, 62), (47, 63), (47, 64), (47, 65), (47, 66), (47, 67), (47, 68), (47, 69), (47, 70), (47, 71), (47, 72), (47, 73), (47, 74), (47, 75), (47, 76), (47, 77), (47, 78), (47, 79), (47, 80), (47, 81), (47, 82), (47, 83), (47, 84), (48, 84), (49, 84), (50, 84), (51, 84), (52, 84), (53, 84), (54, 84), (55, 84), (56, 84), (57, 84), (58, 84), (59, 84), (60, 84), (61, 84), (62, 84), (63, 84), (63, 85), (63, 86), (63, 87), (63, 88), (63, 89), (63, 90), (63, 91), (63, 92), (64, 92), (65, 92), (66, 92), (67, 92), (68, 92), (69, 92), (70, 92), (71, 92), (72, 92), (73, 92), (74, 92), (75, 92), (76, 92), (76, 91), (76, 90), (76, 89), (76, 88), (76, 87), (76, 86), (76, 85), (76, 84), (76, 83), (76, 82), (76, 81), (76, 80), (76, 79), (76, 78), (76, 77), (76, 76), (77, 76), (78, 76), (79, 76), (80, 76), (81, 76), (82, 76), (83, 76), (84, 76), (85, 76), (86, 76), (87, 76), (88, 76), (89, 76), (90, 76), (91, 76), (92, 76), (92, 75), (92, 74), (92, 73), (92, 72), (92, 71), (91, 71), (90, 71), (89, 71), (89, 70), (88, 70), (88, 69), (87, 69), (87, 68), (86, 68), (86, 67)]}\n",
      " {'map_name': 'maze_5.png', 'start': (71, 30), 'goal': (55, 33), 'trajectory': [(71, 29), (71, 28), (71, 27), (71, 26), (71, 25), (71, 24), (71, 23), (72, 23), (73, 23), (74, 23), (75, 23), (76, 23), (77, 23), (78, 23), (79, 23), (80, 23), (81, 23), (82, 23), (83, 23), (84, 23), (85, 23), (86, 23), (87, 23), (88, 23), (89, 23), (90, 23), (91, 23), (92, 23), (92, 24), (92, 25), (92, 26), (92, 27), (92, 28), (92, 29), (92, 30), (92, 31), (92, 32), (92, 33), (92, 34), (92, 35), (92, 36), (91, 36), (90, 36), (89, 36), (88, 36), (87, 36), (86, 36), (85, 36), (84, 36), (83, 36), (82, 36), (81, 36), (80, 36), (79, 36), (78, 36), (77, 36), (76, 36), (75, 36), (74, 36), (73, 36), (72, 36), (71, 36), (70, 36), (69, 36), (68, 36), (67, 36), (66, 36), (65, 36), (64, 36), (63, 36), (62, 36), (61, 36), (60, 36), (59, 36), (58, 36), (57, 36), (56, 36), (55, 36), (55, 35), (55, 34), (55, 33)]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thakk\\AppData\\Local\\Temp\\ipykernel_11120\\3930722311.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  actual_noises = pad_sequence([torch.tensor(traj) - torch.tensor(trajectories_list[i]) for i, traj in enumerate(noisy_trajectories)], batch_first=True).float()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (140) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 194\u001b[0m\n\u001b[0;32m    191\u001b[0m actual_noises \u001b[38;5;241m=\u001b[39m pad_sequence([torch\u001b[38;5;241m.\u001b[39mtensor(traj) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(trajectories_list[i]) \u001b[38;5;28;01mfor\u001b[39;00m i, traj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(noisy_trajectories)], batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# Train the noise prediction network\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m noise_prediction_model \u001b[38;5;241m=\u001b[39m train_noise_prediction_network(latent_embeddings, start_goals, noisy_trajectories_tensor, actual_noises)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# Create and use the DiPPeR model\u001b[39;00m\n\u001b[0;32m    197\u001b[0m dipper_model \u001b[38;5;241m=\u001b[39m DiPPeR(noise_prediction_model)\n",
      "Cell \u001b[1;32mIn[1], line 129\u001b[0m, in \u001b[0;36mtrain_noise_prediction_network\u001b[1;34m(latent_embeddings, start_goals, noisy_trajectories, actual_noises, epochs, lr)\u001b[0m\n\u001b[0;32m    126\u001b[0m noisy_traj \u001b[38;5;241m=\u001b[39m noisy_trajectories[i]\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# [1, 2, T] and convert to float32\u001b[39;00m\n\u001b[0;32m    127\u001b[0m actual_noise \u001b[38;5;241m=\u001b[39m actual_noises[i]\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# [1, 2, T] and convert to float32\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(noisy_traj, cond\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    130\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, actual_noise)  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m    132\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\thakk\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\thakk\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 108\u001b[0m, in \u001b[0;36mTemporalCNNFiLM.forward\u001b[1;34m(self, x, cond)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cond):\n\u001b[0;32m    107\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[1;32m--> 108\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilm1(x, cond))\n\u001b[0;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n\u001b[0;32m    110\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilm2(x, cond))\n",
      "File \u001b[1;32mc:\\Users\\thakk\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\thakk\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 94\u001b[0m, in \u001b[0;36mFiLM.forward\u001b[1;34m(self, x, cond)\u001b[0m\n\u001b[0;32m     92\u001b[0m gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma(cond)\n\u001b[0;32m     93\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta(cond)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gamma \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m beta\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (140) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Load images function\n",
    "def load_images(folder):\n",
    "    images = []\n",
    "    transform = transforms.Compose([transforms.Resize((100, 100)), transforms.ToTensor()])\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.png'):\n",
    "            img = Image.open(os.path.join(folder, filename)).convert('RGB')\n",
    "            img = transform(img)\n",
    "            images.append(img)\n",
    "    return torch.stack(images)\n",
    "\n",
    "# Load the .npy file to inspect its contents\n",
    "trajectories_data = np.load('trajectories_3.npy', allow_pickle=True)\n",
    "print(type(trajectories_data))\n",
    "print(trajectories_data.shape)\n",
    "print(trajectories_data)\n",
    "\n",
    "# Define the Resnet_Visual_Encoder function\n",
    "class ResNet18VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18VisualEncoder, self).__init__()\n",
    "        self.resnet18 = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.resnet18 = nn.Sequential(*list(self.resnet18.children())[:-2])\n",
    "\n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.resnet18(images)\n",
    "        return embeddings\n",
    "\n",
    "def Resnet_Visual_Encoder(images):\n",
    "    encoder = ResNet18VisualEncoder()\n",
    "    latent_embeddings = encoder(images)\n",
    "    return latent_embeddings\n",
    "\n",
    "# Define the Pathl function\n",
    "def Pathl(data):\n",
    "    trajectories = [d['trajectory'] for d in data]\n",
    "    lengths = [len(traj) for traj in trajectories]\n",
    "    return lengths\n",
    "\n",
    "# Define the adding_Gaussian_noise function\n",
    "# Cosine noise scheduler\n",
    "def cosine_noise_scheduler(T, beta_start=0.0001, beta_end=0.02):\n",
    "    return np.cos((np.linspace(0, T, T) + 0.008) / (1.008) * np.pi * 0.5) ** 2 * (beta_end - beta_start) + beta_start\n",
    "\n",
    "# Adding Gaussian noise function\n",
    "def adding_Gaussian_noise(lengths, trajectories):\n",
    "    noisy_trajectories = []\n",
    "    T = 1000  # Number of diffusion steps\n",
    "    beta_schedule = cosine_noise_scheduler(T)\n",
    "    for i, length in enumerate(lengths):\n",
    "        trajectory = trajectories[i]\n",
    "        noise = np.random.normal(0, 1, (length, 2))\n",
    "        noisy_trajectory = np.array(trajectory) + noise * beta_schedule[:length, None]\n",
    "        noisy_trajectories.append(noisy_trajectory)\n",
    "    return noisy_trajectories\n",
    "\n",
    "# Define the noise_prediction_network function\n",
    "# Noise prediction network class\n",
    "class NoisePredictionNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(NoisePredictionNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# FiLM layer class\n",
    "class FiLM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(FiLM, self).__init__()\n",
    "        self.gamma = nn.Linear(in_channels, out_channels)\n",
    "        self.beta = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        gamma = self.gamma(cond)\n",
    "        beta = self.beta(cond)\n",
    "        return gamma * x + beta\n",
    "\n",
    "# Temporal CNN with FiLM class\n",
    "class TemporalCNNFiLM(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(TemporalCNNFiLM, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)\n",
    "        self.film1 = FiLM(latent_dim, 64)\n",
    "        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n",
    "        self.film2 = FiLM(latent_dim, 32)\n",
    "        self.conv3 = nn.Conv1d(32, 2, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(self.film1(x, cond))\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(self.film2(x, cond))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "# Train noise prediction network function\n",
    "def train_noise_prediction_network(latent_embeddings, start_goals, noisy_trajectories, actual_noises, epochs=2, lr=0.01):\n",
    "    latent_dim = latent_embeddings.shape[1] * latent_embeddings.shape[2] * latent_embeddings.shape[3]\n",
    "    model = TemporalCNNFiLM(input_dim=2, latent_dim=latent_dim + 4)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(noisy_trajectories)):\n",
    "            latent_embedding = latent_embeddings[i].view(-1).float()  # Flatten to 1D tensor and convert to float32\n",
    "            start_goal_flat = start_goals[i].view(-1).float()  # Flatten start_goal to 1D tensor and convert to float32\n",
    "            cond = torch.cat((latent_embedding, start_goal_flat), dim=0).float()  # Combine latents and start_goal\n",
    "            noisy_traj = noisy_trajectories[i].permute(1, 0).unsqueeze(0).float()  # [1, 2, T] and convert to float32\n",
    "            actual_noise = actual_noises[i].permute(1, 0).unsqueeze(0).float()  # [1, 2, T] and convert to float32\n",
    "\n",
    "            predictions = model(noisy_traj, cond.unsqueeze(0))\n",
    "            loss = criterion(predictions, actual_noise)  # Compute loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# DiPPeR model class using Temporal CNN with FiLM\n",
    "class DiPPeR(nn.Module):\n",
    "    def __init__(self, noise_prediction_network):\n",
    "        super(DiPPeR, self).__init__()\n",
    "        self.noise_prediction_network = noise_prediction_network\n",
    "\n",
    "    def forward(self, O, noisy_trajectory, path_length, start_goal):\n",
    "        denoised_trajectory = noisy_trajectory\n",
    "        latent_embedding = O.view(-1).float()  # Flatten latent embedding and convert to float32\n",
    "        start_goal_flat = start_goal.view(-1).float()  # Flatten start_goal and convert to float32\n",
    "        cond = torch.cat((latent_embedding, start_goal_flat), dim=0).float()  # Combine latents and start_goal\n",
    "\n",
    "        for t in reversed(range(path_length)):\n",
    "            traj_point = denoised_trajectory[t].view(-1).float()  # Flatten trajectory point and convert to float32\n",
    "            traj_point = traj_point.unsqueeze(0).unsqueeze(0)  # [1, 1, 2]\n",
    "            noise_pred = self.noise_prediction_network(traj_point.permute(0, 2, 1), cond.unsqueeze(0))\n",
    "            denoised_trajectory[t] = denoised_trajectory[t] - noise_pred.squeeze(0).permute(1, 0)\n",
    "\n",
    "        return denoised_trajectory\n",
    "\n",
    "# Load data\n",
    "images = load_images('./maze_maps3')\n",
    "latent_embeddings = Resnet_Visual_Encoder(images)\n",
    "\n",
    "# Load trajectories data\n",
    "data = np.load('trajectories_3.npy', allow_pickle=True)\n",
    "\n",
    "start_goals = []\n",
    "trajectories_list = []\n",
    "for d in data:\n",
    "    start = torch.tensor(d['start'])\n",
    "    goal = torch.tensor(d['goal'])\n",
    "    start_goal = torch.cat((start, goal))  # Concatenate start and goal tensors\n",
    "    start_goals.append(start_goal)\n",
    "    trajectories_list.append(torch.tensor(d['trajectory']))\n",
    "\n",
    "# Calculate lengths and add Gaussian noise\n",
    "lengths = Pathl(data)\n",
    "noisy_trajectories = adding_Gaussian_noise(lengths, trajectories_list)\n",
    "\n",
    "# Ensure all tensors have the same number of elements\n",
    "if len(latent_embeddings) != len(start_goals):\n",
    "    print(f\"Mismatch in the number of images ({len(latent_embeddings)}) and trajectories ({len(start_goals)}). Adjusting...\")\n",
    "\n",
    "    # Adjust latent embeddings to match the number of start_goals\n",
    "    latent_embeddings = latent_embeddings[:len(start_goals)]\n",
    "\n",
    "assert len(latent_embeddings) == len(start_goals) == len(noisy_trajectories), \"Mismatch in number of elements\"\n",
    "\n",
    "# Convert data to tensors\n",
    "noisy_trajectories_tensor = pad_sequence([torch.tensor(traj) for traj in noisy_trajectories], batch_first=True).float()\n",
    "actual_noises = pad_sequence([torch.tensor(traj) - torch.tensor(trajectories_list[i]) for i, traj in enumerate(noisy_trajectories)], batch_first=True).float()\n",
    "\n",
    "# Train the noise prediction network\n",
    "noise_prediction_model = train_noise_prediction_network(latent_embeddings, start_goals, noisy_trajectories_tensor, actual_noises)\n",
    "\n",
    "# Create and use the DiPPeR model\n",
    "dipper_model = DiPPeR(noise_prediction_model)\n",
    "final_trajectories = [dipper_model(latent_embeddings[i], noisy_trajectories_tensor[i], lengths[i], start_goals[i]) for i in range(len(noisy_trajectories_tensor))]\n",
    "\n",
    "# Plot images with trajectories\n",
    "def plot_images_with_trajectories(images, trajectories, output_folder='output_images'):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    transform = transforms.ToPILImage()\n",
    "\n",
    "    for i, (image, trajectory) in enumerate(zip(images, trajectories)):\n",
    "        img = transform(image)\n",
    "        plt.figure()\n",
    "        plt.imshow(img)\n",
    "\n",
    "        trajectory = trajectory.detach().cpu().numpy()\n",
    "        plt.plot(trajectory[:, 0], trajectory[:, 1], marker='o', color='r', linestyle='-')\n",
    "\n",
    "        plt.title(f'Image with Denoised Trajectory {i}')\n",
    "        plt.axis('off')\n",
    "        plt.savefig(os.path.join(output_folder, f'image_with_trajectory_{i}.png'))\n",
    "        plt.close()\n",
    "\n",
    "plot_images_with_trajectories(images, final_trajectories)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
